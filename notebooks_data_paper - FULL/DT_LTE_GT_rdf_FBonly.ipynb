{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f4df3e8-3c77-422f-83ee-8af6e2d5eea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "from subprocess import check_output\n",
    "import shutil\n",
    "import rdflib\n",
    "from multiprocessing import Process, Manager, Queue\n",
    "import multiprocessing\n",
    "\n",
    "from rule import *\n",
    "from amie import *\n",
    "from experiment import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55f158aa-318d-4902-bf5c-e10cc9278b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_source_FB = \"./../../FB15k_mail/\"\n",
    "root_source_DB = \"./../../DB15k_mail/\"\n",
    "\n",
    "root_data_FB = \"./../../Data_mail/FB_LTE_GT_\"\n",
    "root_data_DB = \"./../../Data_mail/DB_LTE_GT_\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba56f36-0b8b-48b5-bc3f-871074efd132",
   "metadata": {},
   "source": [
    "# FB15K "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d443c858-86b5-440d-ba2a-d53fbd1d09aa",
   "metadata": {},
   "source": [
    "### Prepare the data\n",
    "\n",
    "As we need a baseline for Amie, we are not going to change the value of anything. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26367f12-8c1c-4781-a60d-3d6c265e823c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(root_source_FB+\"numericals.txt\", \"r\")\n",
    "\n",
    "numerical_predicate = set()\n",
    "\n",
    "for predicate in data:\n",
    "    numerical_predicate.add(predicate.split(\"\\n\")[0])\n",
    "    \n",
    "data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48a31c9e-d0ef-4975-9101-b355e827c59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.arange(0.25,1,0.25) #[0.25, 0.50, 0.75]\n",
    "thresholds_str = [\"-\"+(str(int(i*100))) for i in thresholds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c2ba460-c1ca-4055-8ca9-e09bf10db405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(X, f, threshold, comparator):\n",
    "    f.write(f\"<http:{X['Subject']}>\\t{X['Predicate'][:-1]}_LTE_{threshold}>\\t<http:/{X['Object']<=comparator}>\\n\")\n",
    "    f.write(f\"<http:{X['Subject']}>\\t{X['Predicate'][:-1]}_GT_{threshold}>\\t<http:/{X['Object']>comparator}>\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a88239e-a71b-4031-9ee4-702b54d93eb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = open(root_source_FB+\"train.txt\", \"r\")\n",
    "f = open(root_data_FB+\"train.tsv\", \"w\")\n",
    "\n",
    "dic_predicate = {}\n",
    "\n",
    "for line in data:\n",
    "    line_split = line.split(\"\\n\")[0].split(\"\\t\")\n",
    "    if line_split[1] in numerical_predicate:\n",
    "        line_split[2] = float(line_split[2]) \n",
    "        line_split = tuple(line_split)\n",
    "        if line_split[1] in dic_predicate.keys():\n",
    "            dic_predicate[line_split[1]].add(line_split)\n",
    "        else : \n",
    "            dic_predicate[line_split[1]] = {line_split}\n",
    "    else:\n",
    "        for var in line_split:\n",
    "            f.write('<http:'+var+'>\\t')\n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "data.close()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0be17586-1335-4bb9-bfc1-def4fae71c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(root_data_FB+\"train.tsv\", \"a\")\n",
    "\n",
    "for key in dic_predicate.keys():\n",
    "    tp_df = pd.DataFrame.from_dict(dic_predicate[key]).rename(columns={0: \"Subject\", 1: \"Predicate\", 2: \"Object\"})\n",
    "    tp_df_describe = tp_df[\"Object\"].quantile(thresholds)\n",
    "    for threshold in thresholds: \n",
    "        tp_df.apply(write_file, args=(f, threshold, tp_df_describe[threshold]), axis=1)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5ee43d-f386-45ce-9f91-4bcec2a029ee",
   "metadata": {},
   "source": [
    "### Launch Amie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e0e7a85-cde5-4f70-be50-30cf796a339e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-35c58594d97f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'java -jar ./../amie3.jar {root_data_FB+\"train.tsv\"}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mres_parsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_amie\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[0m\u001b[1;32m    416\u001b[0m                **kwargs).stdout\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    496\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTimeoutExpired\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1013\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stdin_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1015\u001b[0;31m                 \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1016\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "res = check_output(f'java -jar ./../amie3.jar {root_data_FB+\"train.tsv\"}', shell=True)\n",
    "\n",
    "res_parsed = parse_amie(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917d3d9a-6556-4f22-acec-d17a69094871",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9464d25-1536-431c-a949-351dbe24df6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# res_parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966b15ff-d8aa-492c-98fb-c4380e35642f",
   "metadata": {},
   "source": [
    "### Clean rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc54516b-6da0-4f0d-b3d2-c022c82b0b34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res_parsed_clean = res_parsed.copy()\n",
    "for i in res_parsed:\n",
    "    if len(i.hypotheses) == 1:\n",
    "        if (i.hypotheses[0].predicate.split(\"LTE\")[0] == i.conclusion.predicate.split(\"LTE\")[0]) and len(i.hypotheses[0].predicate.split(\"LTE\")) == 2:\n",
    "#             print(i)\n",
    "            res_parsed_clean.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beebc365-62e1-44f7-b86b-6c5d53f1f901",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res_parsed_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead04fd0-3a4e-41b5-8c11-37e9029405af",
   "metadata": {},
   "source": [
    "### Number of numericals in the rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57b4760-5e79-48ba-a938-ec9b088c55c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_f = open(\"LTE_GT_num_rules.txt\", \"w\")\n",
    "symb_f = open(\"LTE_GT_symb_rules.txt\", \"w\")\n",
    "\n",
    "def predicate_is_numerical(atom, numerical_predicate):\n",
    "    return \"LTE\" in atom.predicate or \"GT\" in atom.predicate or atom.objectD.isdigit()\n",
    "\n",
    "rule_with_numerical_in_hyp = 0\n",
    "rule_with_numerical_in_conc = 0\n",
    "rule_with_numerical = 0\n",
    "\n",
    "for rule in res_parsed:\n",
    "    num = False\n",
    "    for hyp in rule.hypotheses:\n",
    "        if predicate_is_numerical(hyp, numerical_predicate):\n",
    "            rule_with_numerical_in_hyp+=1\n",
    "            num = True\n",
    "            break\n",
    "    if predicate_is_numerical(rule.conclusion, numerical_predicate):\n",
    "        rule_with_numerical_in_conc+=1\n",
    "        num=True\n",
    "    if num:\n",
    "        rule_with_numerical += 1\n",
    "        num_f.write(str(rule)+\"\\n\")\n",
    "    else:\n",
    "        symb_f.write(str(rule)+\"\\n\")\n",
    "    \n",
    "print(\"Rule with numerical : \", rule_with_numerical)\n",
    "print(\"Rule without numerical : \", len(res_parsed) - rule_with_numerical)\n",
    "print(\"Rule with numerical in hypotheses : \", rule_with_numerical_in_hyp)\n",
    "print(\"Rule with numerical in conclusion : \", rule_with_numerical_in_conc)\n",
    "\n",
    "num_f.close()\n",
    "symb_f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3a98b9-73e3-4f5c-b7aa-a4e9731408f9",
   "metadata": {},
   "source": [
    "### Test the rules through test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a24423f-560e-4af8-bb18-5e5d56f557cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(root_data_FB+\"train.tsv\", \"r\")\n",
    "f = open(root_data_FB+\"train_rdflib.nt\", \"w\")\n",
    "\n",
    "for line in data:\n",
    "    f.write(line.split(\"\\n\")[0]+\" . \\n\")\n",
    "\n",
    "data.close()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5569b4-3f47-4621-b540-fc8b8b281fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(root_source_FB+\"test.txt\", \"r\")\n",
    "\n",
    "set_instances_to_predict = set()\n",
    "\n",
    "for line in data:\n",
    "    line_splited = line.split(\"\\n\")[0].split(\"\\t\")\n",
    "    for i in range(len(line_splited)):\n",
    "        line_splited[i] = \"<http:\"+line_splited[i]+\">\"\n",
    "    set_instances_to_predict.add(tuple(line_splited))\n",
    "        \n",
    "data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf28bd7a-270c-49e7-add2-085d66a3fd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set_instances_to_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb040cec-27c2-4ebb-a8f7-290e556b35e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_associated_to_query = {}\n",
    "\n",
    "for rule in res_parsed:\n",
    "    if rule.conclusion.predicate in rules_associated_to_query.keys():\n",
    "        rules_associated_to_query[rule.conclusion.predicate].add(rule)\n",
    "    else:\n",
    "        rules_associated_to_query[rule.conclusion.predicate] = set()\n",
    "        rules_associated_to_query[rule.conclusion.predicate].add(rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a01425c-9d1a-402f-925a-dad5cb6eda71",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = rdflib.Graph()\n",
    "g.parse(root_data_FB+\"train_rdflib.nt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f224508f-b9d3-475b-beda-48bc4bddc84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_rules_nicely(rule):\n",
    "    toprint = \"\"\n",
    "    for hyp in rule.hypotheses:\n",
    "        toprint += hyp.predicate+\" & \"\n",
    "    toprint = toprint[:-2]\n",
    "    toprint += \"=>\"+rule.conclusion.predicate\n",
    "    print(toprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d011b1a3-592a-43fb-b25e-c04535b15258",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def predict_instance(name, g, queue, prediction_per_instance_man, rules_associated_to_query, cpt, total_length, print_advancment):\n",
    "    \n",
    "    print(f\"Process n°{name} : Launched\")\n",
    "    \n",
    "    while not queue.empty():\n",
    "        \n",
    "        instance = queue.get()\n",
    "        \n",
    "        dict_tp = {}\n",
    "        \n",
    "        if instance[1] in rules_associated_to_query.keys():\n",
    "            for rule in rules_associated_to_query[instance[1]]:\n",
    "                try:\n",
    "                    qres = g.query(create_query(rule, instance[2]))\n",
    "\n",
    "                    set_res = set()\n",
    "                    bool_res = False\n",
    "                    for row in qres:\n",
    "                        bool_res = True\n",
    "                        set_res.add(str(row.a))\n",
    "\n",
    "                    if bool_res:\n",
    "                        dict_tp[rule] = set_res\n",
    "            \n",
    "                except:\n",
    "                    print(create_query(rule, instance[2]))\n",
    "                    \n",
    "            prediction_per_instance_man[instance] = dict_tp\n",
    "            \n",
    "        else:\n",
    "            prediction_per_instance_man[instance] = {}\n",
    "        \n",
    "        cpt.value += 1\n",
    "        if (cpt.value/total_length > print_advancment.value):\n",
    "            print(print_advancment.value*100, \"%\")\n",
    "            print_advancment.value+=0.1\n",
    "        \n",
    "    print(f\"Process n°{name} : Finished\")   \n",
    "    \n",
    "q = Queue()\n",
    "prediction_per_instance = {}\n",
    "\n",
    "for instance in list(set_instances_to_predict):\n",
    "    q.put(instance)\n",
    "\n",
    "size_queue = q.qsize()\n",
    "\n",
    "print(\"Queue finished\")\n",
    "\n",
    "with Manager() as manager:\n",
    "\n",
    "    processes_to_create = multiprocessing.cpu_count()-1\n",
    "    processes = list()\n",
    "\n",
    "    prediction_per_instance_man = manager.dict()\n",
    "    cpt = manager.Value(\"cpt\",0)\n",
    "    print_advancment = manager.Value(\"print_advancment\",0)\n",
    "\n",
    "    for name in range(processes_to_create):\n",
    "        x = Process(target=predict_instance, args=(name, g, q, prediction_per_instance_man, rules_associated_to_query, cpt, size_queue, print_advancment))\n",
    "        processes.append(x)\n",
    "        x.start()\n",
    "        \n",
    "    for index, process in enumerate(processes):\n",
    "        process.join()\n",
    "    \n",
    "    print(\"copy\")\n",
    "    \n",
    "    df_prediction = {}\n",
    "    \n",
    "    cpt = 0\n",
    "    advcement = 0.1\n",
    "    total_length = len(prediction_per_instance_man)\n",
    "    \n",
    "    del g\n",
    "\n",
    "    for prediction_instance in prediction_per_instance_man:\n",
    "        df_rules = {}\n",
    "        cpt += 1\n",
    "        for rule in prediction_per_instance_man[prediction_instance]:\n",
    "            df_rules[rule] = [set(prediction_per_instance_man[prediction_instance][rule]), rule.stdConfidence, rule.pcaConfidence]\n",
    "        df_prediction[prediction_instance] = pd.DataFrame.from_dict(df_rules, orient=\"index\", columns=[\"Prediction\", \"Std Confidence\", \"Pca Confidence\"])\n",
    "        \n",
    "        if (cpt/total_length > advcement):\n",
    "            print(advcement *100, \"%\")\n",
    "            advcement+=0.1\n",
    "        \n",
    "    print(\"----- Democracy -----\")\n",
    "    hit_at(df_prediction, democracy, 1)\n",
    "    hit_at(df_prediction, democracy, 5)\n",
    "    hit_at(df_prediction, democracy, 10)\n",
    "    hit_at(df_prediction, democracy, 1000)\n",
    "\n",
    "    print(\"----- Expert -----\")\n",
    "    hit_at(df_prediction, expert, 1)\n",
    "    hit_at(df_prediction, expert, 5)\n",
    "    hit_at(df_prediction, expert, 10)\n",
    "    hit_at(df_prediction, expert, 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
